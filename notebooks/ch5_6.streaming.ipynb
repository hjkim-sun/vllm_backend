{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행 위치: Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. vllm 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LsSf https://astral.sh/uv/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 61063,
     "status": "ok",
     "timestamp": 1767612370899,
     "user": {
      "displayName": "김현진",
      "userId": "15291438294537760967"
     },
     "user_tz": -540
    },
    "id": "ussykECbKUho"
   },
   "outputs": [],
   "source": [
    "!uv pip install --system vllm==0.11.0 torch==2.8.0 transformers==4.57.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 로드\n",
    " - 모델 로드 중 실패 발생시 제보 부탁드립니다. (Colab 기본 라이브러리 버전과 충돌 발생 가능성 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1767612494602,
     "user": {
      "displayName": "김현진",
      "userId": "15291438294537760967"
     },
     "user_tz": -540
    },
    "id": "dFq7s3nRKYCs"
   },
   "outputs": [],
   "source": [
    "!vllm serve Qwen/Qwen3-1.7B > vllm_server.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1767612838626,
     "user": {
      "displayName": "김현진",
      "userId": "15291438294537760967"
     },
     "user_tz": -540
    },
    "id": "WOvINeUYKwmU",
    "outputId": "cb32872d-6b4f-4185-b129-aece66e98355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 11:28:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "2026-01-05 11:28:19.041318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767612499.062076    1977 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767612499.070922    1977 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767612499.087509    1977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767612499.087536    1977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767612499.087539    1977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767612499.087542    1977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-05 11:28:19.092535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:28:28 [api_server.py:1839] vLLM API server version 0.11.0\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:28:28 [utils.py:233] non-default args: {'model_tag': 'Qwen/Qwen3-0.6B'}\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:28:48 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m WARNING 01-05 11:28:48 [model.py:1682] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m WARNING 01-05 11:28:48 [model.py:1733] Casting torch.bfloat16 to torch.float16.\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:28:48 [model.py:1510] Using max model len 40960\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:28:51 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 01-05 11:28:59 [__init__.py:216] Automatically detected platform cuda.\n",
      "2026-01-05 11:29:00.409207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767612540.442147    2185 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767612540.455172    2185 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767612540.481656    2185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767612540.481688    2185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767612540.481706    2185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767612540.481709    2185 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:09 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:09 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-0.6B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m ERROR 01-05 11:29:10 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:11 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m WARNING 01-05 11:29:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:12 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-0.6B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:12 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:12 [cuda.py:372] Using FlexAttention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:13 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:25 [weight_utils.py:413] Time spent downloading weights for Qwen/Qwen3-0.6B: 11.977661 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:25 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m \rLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m \rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.22s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m \rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.23s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:27 [default_loader.py:267] Loading weights took 1.25 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:28 [gpu_model_runner.py:2653] Model loading took 1.1201 GiB and 14.419438 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:36 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5c259b38e1/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:36 [backends.py:559] Dynamo bytecode transform time: 8.02 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m [rank0]:W0105 11:29:38.442000 2185 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:29:42 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:30:06 [backends.py:218] Compiling a graph for dynamic shape takes 29.52 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:30:14 [monitor.py:34] torch.compile takes 37.54 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:30:16 [gpu_worker.py:298] Available KV cache memory: 10.72 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:30:16 [kv_cache_utils.py:1087] GPU KV cache size: 100,384 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:30:16 [kv_cache_utils.py:1091] Maximum concurrency for 40,960 tokens per request: 2.45x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m WARNING 01-05 11:30:16 [gpu_model_runner.py:3663] CUDAGraphMode.FULL_AND_PIECEWISE is not supported with FlexAttentionMetadataBuilder backend (support: AttentionCGSupport.NEVER); setting cudagraph_mode=PIECEWISE because attention is compiled piecewise\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m \rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 2/67 [00:00<00:03, 16.45it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:03, 16.49it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|█         | 7/67 [00:00<00:03, 18.37it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|█▍        | 10/67 [00:00<00:02, 19.47it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:00<00:02, 20.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▍       | 16/67 [00:00<00:02, 20.74it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 19/67 [00:00<00:02, 21.53it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 22/67 [00:01<00:02, 22.00it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 25/67 [00:01<00:01, 22.60it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 28/67 [00:01<00:01, 22.53it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▋     | 31/67 [00:01<00:01, 23.17it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 34/67 [00:01<00:01, 23.65it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▌    | 37/67 [00:01<00:01, 23.66it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|█████▉    | 40/67 [00:01<00:01, 23.14it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:01<00:01, 22.49it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 46/67 [00:02<00:00, 21.70it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 49/67 [00:02<00:00, 19.73it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 52/67 [00:02<00:00, 20.26it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 55/67 [00:02<00:00, 20.88it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 58/67 [00:02<00:00, 21.24it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 61/67 [00:02<00:00, 21.47it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 64/67 [00:02<00:00, 21.48it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 17.50it/s]\rCapturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 20.68it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:30:21 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.45 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2185)\u001b[0;0m INFO 01-05 11:30:21 [core.py:210] init engine (profile, create kv cache, warmup model) took 53.13 seconds\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:22 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 6274\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:22 [api_server.py:1634] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m WARNING 01-05 11:30:23 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:34] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /docs, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /redoc, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO 01-05 11:30:23 [launcher.py:42] Route: /metrics, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO:     Started server process [1977]\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=1977)\u001b[0;0m INFO:     Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "!cat vllm_server.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05xuF9ScMXxn"
   },
   "source": [
    "# 3. Streaming 요청"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrGoOyfLMfTA"
   },
   "source": [
    "## 3-1. curl 명령"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2357,
     "status": "ok",
     "timestamp": 1767615205075,
     "user": {
      "displayName": "김현진",
      "userId": "15291438294537760967"
     },
     "user_tz": -540
    },
    "id": "fJRGy1d3MsQl",
    "outputId": "9e1f9d49-c0c3-407a-bdb5-c4eabb55ac77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> POST /v1/chat/completions HTTP/1.1\r\n",
      "> Host: localhost:8000\r\n",
      "> User-Agent: curl/7.81.0\r\n",
      "> Accept: */*\r\n",
      "> Content-Type: application/json\r\n",
      "> Content-Length: 165\r\n",
      "> \r\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\r\n",
      "< date: Mon, 05 Jan 2026 12:13:24 GMT\r\n",
      "< server: uvicorn\r\n",
      "< content-type: text/event-stream; charset=utf-8\r\n",
      "< transfer-encoding: chunked\r\n",
      "< \r\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}],\"prompt_token_ids\":null}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"<think>\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\\n\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Okay\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" the\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" user\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" is\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" asking\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" \\\"\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"San\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" Francisco\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" is\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"...\\\"\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" which\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" seems\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" bit\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" ambiguous\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" Maybe\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" they\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" want\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" to\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" know\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" the\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" role\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" or\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" significance\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" of\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" San\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" Francisco\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" Let\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" me\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" break\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" this\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" down\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\\n\\n\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"First\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" San\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" Francisco\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" is\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" city\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" in\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" California\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" known\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" for\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" its\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" history\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" culture\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" and\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" landmarks\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" The\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" user\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" might\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" be\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" referring\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" to\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" its\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" cultural\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" aspects\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" historical\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" importance\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" or\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" maybe\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" specific\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" activity\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" But\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" since\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" the\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" question\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" is\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" too\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" vague\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\",\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" I\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" should\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" provide\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" a\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" general\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" overview\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\".\\n\\n\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"I\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" need\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" to\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" mention\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" key\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" points\"},\"logprobs\":null,\"finish_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: {\"id\":\"chatcmpl-06f9d8c42ac84694a4c489dcde40650b\",\"object\":\"chat.completion.chunk\",\"created\":1767615204,\"model\":\"Qwen/Qwen3-0.6B\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" like\"},\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null,\"token_ids\":null}]}\n",
      "\n",
      "data: [DONE]\n",
      "\n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{ \\\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"San Francisco is a\"}], \\\n",
    "        \"max_tokens\": 100, \\\n",
    "        \"temperature\": 0.8, \\\n",
    "        \"stream\": \"true\" \\\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QawhXx13Mi6i"
   },
   "source": [
    "## 3-2. requests 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6694,
     "status": "ok",
     "timestamp": 1767614036684,
     "user": {
      "displayName": "김현진",
      "userId": "15291438294537760967"
     },
     "user_tz": -540
    },
    "id": "H0qII8eANCwX",
    "outputId": "e7d25711-60e7-473a-d25e-e893ce96ff6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "\n",
      "Okay\n",
      ",\n",
      " the\n",
      " user\n",
      " is\n",
      " asking\n",
      " \"\n",
      "San\n",
      " Francisco\n",
      " is\n",
      " a\n",
      "...\"\n",
      " which\n",
      " seems\n",
      " to\n",
      " be\n",
      " a\n",
      " bit\n",
      " of\n",
      " a\n",
      " vague\n",
      " question\n",
      ".\n",
      " First\n",
      ",\n",
      " I\n",
      " need\n",
      " to\n",
      " figure\n",
      " out\n",
      " what\n",
      " they\n",
      "'re\n",
      " really\n",
      " asking\n",
      ".\n",
      " Maybe\n",
      " they\n",
      " want\n",
      " to\n",
      " know\n",
      " what\n",
      " San\n",
      " Francisco\n",
      " is\n",
      " in\n",
      " general\n",
      "?\n",
      " Or\n",
      " perhaps\n",
      " they\n",
      "'re\n",
      " looking\n",
      " for\n",
      " information\n",
      " on\n",
      " a\n",
      " specific\n",
      " topic\n",
      " related\n",
      " to\n",
      " San\n",
      " Francisco\n",
      ".\n",
      "\n",
      "\n",
      "Since\n",
      " the\n",
      " user\n",
      " hasn\n",
      "'t\n",
      " provided\n",
      " more\n",
      " context\n",
      ",\n",
      " I\n",
      " should\n",
      " start\n",
      " by\n",
      " acknowledging\n",
      " the\n",
      " ambiguity\n",
      ".\n",
      " I\n",
      " can\n",
      " explain\n",
      " that\n",
      " San\n",
      " Francisco\n",
      " is\n",
      " a\n",
      " city\n",
      " in\n",
      " California\n",
      ",\n",
      " known\n",
      " for\n",
      " its\n",
      " history\n",
      ",\n",
      " culture\n",
      ",\n",
      " landmarks\n",
      ",\n",
      " and\n",
      " economic\n",
      " activity\n",
      ".\n",
      " I\n",
      " should\n",
      " mention\n",
      " key\n",
      " points\n",
      " like\n",
      " its\n",
      " role\n",
      " as\n",
      " a\n",
      " major\n",
      " city\n",
      ",\n",
      " famous\n",
      " attractions\n",
      ",\n",
      " and\n",
      " its\n",
      " significance\n",
      " in\n",
      " the\n",
      " tech\n",
      " industry\n",
      ".\n",
      "\n",
      "\n",
      "I\n",
      " should\n",
      " also\n",
      " consider\n",
      " if\n",
      " there\n",
      "'s\n",
      " a\n",
      " specific\n",
      " area\n",
      " they\n",
      "'re\n",
      " interested\n",
      " in\n",
      ".\n",
      " Maybe\n",
      " they\n",
      "'re\n",
      " asking\n",
      " about\n",
      " a\n",
      " particular\n",
      " event\n",
      ",\n",
      " a\n",
      " landmark\n",
      ",\n",
      " or\n",
      " a\n",
      " notable\n",
      " aspect\n",
      " of\n",
      " San\n",
      " Francisco\n",
      ".\n",
      " But\n",
      " since\n",
      " the\n",
      " question\n",
      " is\n",
      " too\n",
      " vague\n",
      ",\n",
      " I\n",
      "'ll\n",
      " keep\n",
      " it\n",
      " general\n",
      " and\n",
      " provide\n",
      " a\n",
      " comprehensive\n",
      " answer\n",
      " that\n",
      " covers\n",
      " its\n",
      " major\n",
      " aspects\n",
      " without\n",
      " assuming\n",
      " their\n",
      " specific\n",
      " focus\n",
      ".\n",
      "\n",
      "</think>\n",
      "\n",
      "\n",
      "\n",
      "San\n",
      " Francisco\n",
      " is\n",
      " a\n",
      " city\n",
      " in\n",
      " California\n",
      ",\n",
      " United\n",
      " States\n",
      ",\n",
      " known\n",
      " for\n",
      " its\n",
      " rich\n",
      " history\n",
      ",\n",
      " cultural\n",
      " landmarks\n",
      ",\n",
      " and\n",
      " economic\n",
      " significance\n",
      ".\n",
      " It\n",
      " is\n",
      " famous\n",
      " for\n",
      " being\n",
      " home\n",
      " to\n",
      " landmarks\n",
      " like\n",
      " the\n",
      " San\n",
      " Francisco\n",
      " Museum\n",
      " of\n",
      " Modern\n",
      " Art\n",
      ",\n",
      " the\n",
      " Golden\n",
      " Gate\n",
      " Bridge\n",
      ",\n",
      " and\n",
      " the\n",
      " San\n",
      " Francisco\n",
      " Chronicle\n",
      ".\n",
      " San\n",
      " Francisco\n",
      " is\n",
      " also\n",
      " a\n",
      " major\n",
      " hub\n",
      " for\n",
      " the\n",
      " tech\n",
      " industry\n",
      ",\n",
      " with\n",
      " Silicon\n",
      " Valley\n",
      " being\n",
      " a\n",
      " key\n",
      " area\n",
      ".\n",
      " Additionally\n",
      ",\n",
      " it\n",
      " plays\n",
      " a\n",
      " significant\n",
      " role\n",
      " in\n",
      " the\n",
      " region\n",
      "'s\n",
      " economy\n",
      ",\n",
      " culture\n",
      ",\n",
      " and\n",
      " global\n",
      " influence\n",
      ".\n",
      " If\n",
      " you\n",
      " have\n",
      " a\n",
      " specific\n",
      " topic\n",
      " in\n",
      " mind\n",
      ",\n",
      " please\n",
      " let\n",
      " me\n",
      " know\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "with requests.post(\n",
    "    \"http://localhost:8000/v1/chat/completions\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"San Francisco is a\"}],\n",
    "        \"prompt\": \"San Francisco is a\",\n",
    "        \"stream\": True\n",
    "    }\n",
    ") as response:\n",
    "    response.raise_for_status()\n",
    "    for line in response.iter_lines(decode_unicode=True):\n",
    "        if line:\n",
    "            if line.startswith(\"data: \"):\n",
    "                try:\n",
    "                    json_data = json.loads(line[len(\"data: \"):])      # \"data: \" 부분은 제외하고 출력\n",
    "                    content = json_data.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\", \"\")\n",
    "                    if content:\n",
    "                        print(content)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O_OzzbYMi8l"
   },
   "source": [
    "## 3-3. openAI 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8349,
     "status": "ok",
     "timestamp": 1767613453126,
     "user": {
      "displayName": "김현진",
      "userId": "15291438294537760967"
     },
     "user_tz": -540
    },
    "id": "XNcRGZduM3Ng",
    "outputId": "64263ece-5413-475d-a090-e29ea9555294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " city with a population of 632,000. How many people are in each row if the city is divided into 20 rows?\n",
      "Answer:\n",
      "\n",
      "To find the number of people in each row, we need to divide the total population by the number of rows. \n",
      "\n",
      "Given:\n",
      "- Total population = 632,000\n",
      "- Number of rows = 20\n",
      "\n",
      "We divide 632,000 by 20.\n",
      "\n",
      "Let me perform the calculation:\n",
      "632,000 ÷ 20 = 31,600\n",
      "\n",
      "So, there are 31,600 people in each row.\n",
      "\n",
      "**Final Answer:** There are 31,600 people in each row.  \n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "\n",
      "Therefore, the answer is $\\boxed{31600}$.\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "\n",
      "**Explanation:**\n",
      "To determine the number of people in each row, we divide the total population by the number of rows. \n",
      "\n",
      "$$\n",
      "\\frac{632,000}{20} = 31,600\n",
      "$$\n",
      "\n",
      "So, there are 31,600 people in each row.  \n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31600}\n",
      "$$\n",
      "**Final Answer**\n",
      "$$\n",
      "\\boxed{31"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model = \"Qwen/Qwen3-0.6B\"\n",
    "prompt = \"San Francisco is a\"\n",
    "max_tokens = 100\n",
    "temperature = 0.8\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "stream_resp = client.completions.create(\n",
    "    model       = model,\n",
    "    prompt      = prompt,\n",
    "    max_tokens  = 400,\n",
    "    temperature = temperature,\n",
    "    stream      = True\n",
    ")\n",
    "for chunk in stream_resp:\n",
    "    if chunk.choices[0].text:\n",
    "        print(chunk.choices[0].text, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSMi15V1Mz8V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMsZT18ep781jrKzSgdQgBe",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
